{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d205e62e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import library first\n",
    "import requests\n",
    "import json\n",
    "from time import sleep\n",
    "\n",
    "# please add bearer_token here\n",
    "bearer_token = \"AAAAAAAAAAAAAAAAAAAAANfsjwEAAAAAB2B3Qrvw%2Bva9BE6eBjThntmEfyY%3DecXMXHlAMedee4N7VRKx6rgw8VkUxaNoOdTgZJdhMu5KKf1mUH\"\n",
    "#bearer_token = \"Add bearer token here\"\n",
    "\n",
    "# set limit for the sample\n",
    "sample_limit =500\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# file to save tweeter response\n",
    "FILE = 'tweet_response.json'\n",
    "# twitter search api url\n",
    "search_url = \"https://api.twitter.com/2/tweets/search/recent\"\n",
    "#search_url = \"https://api.twitter.com/2/tweets/search/all\"\n",
    "\n",
    "#Finnish milk\n",
    "# add query paramters here\n",
    "#change params based on the endpoint you are using\n",
    " \n",
    "query_params = {'query': 'ireland milk -is:retweet',\n",
    "                'tweet.fields': 'author_id', \n",
    "                'max_results': '100',\n",
    "                'next_token': {}}\n",
    "\n",
    "#query_params = {'query': 'ireland milk quality lang:en -is:retweet',\n",
    " #               'tweet.fields': 'author_id', \n",
    "  #              'max_results': '100',\n",
    "   #             'next_token': {}}\n",
    "\n",
    "# function to add request header\n",
    "def bearer_oauth(r):\n",
    "    r.headers = {\"Authorization\": \"Bearer {}\".format(bearer_token)}\n",
    "    r.headers[\"User-Agent\"] = \"v2RecentSearchPython\"\n",
    "    return r\n",
    "# function to send request and recieve response\n",
    "def connect_to_endpoint(url, params):\n",
    "    response = requests.get(url, auth=bearer_oauth, params=params)\n",
    "    #print(response.status_code)\n",
    "    # check if response code is good or bad\n",
    "    if response.status_code != 200:\n",
    "        raise Exception(response.status_code, response.text)\n",
    "    return response.json()\n",
    "\n",
    "# main function to get tweets\n",
    "def get_tweets():\n",
    "    json_response = connect_to_endpoint(search_url, query_params)\n",
    "    \n",
    "    # add try block to catch the error\n",
    "    try:\n",
    "        for post in json_response:\n",
    "            print(json_response)\n",
    "            returnVal = json_response[\"meta\"][\"result_count\"]\n",
    "            #print(\"Tweet count is: %s\" %  returnVal)\n",
    "            if returnVal > 0:\n",
    "                \n",
    "                \n",
    "\n",
    "                json_file = open(FILE, mode='a')\n",
    "                #step up to write the file\n",
    "                json_file.write('{\"data\": [')\n",
    "                # get data from json node\n",
    "                json_file.writelines(json.dumps(json_response[\"data\"])[1:-1])\n",
    "                \n",
    "                \n",
    "\n",
    "                nextToken =\"\"\n",
    "                \n",
    "                # check if meta tag is present the json response\n",
    "                if 'meta' in post:\n",
    "                    for d in json_response[\"meta\"]:\n",
    "                        # print(d)\n",
    "                        if 'next_token' in d:\n",
    "                            #   print(\"Next token value iswewe: %s\" % json_response['meta']['next_token'])\n",
    "                            #print(json_response['meta']['next_token'])\n",
    "                            nextToken=json_response['meta']['next_token']\n",
    "                            #if nextToken ==\"\":\n",
    "                            #print(json_response[\"meta\"][\"oldest_id\"] +\n",
    "                                    #      \"\\n\" + json_response[\"meta\"][\"newest_id\"] +\n",
    "                                    #      \"\\n\" + json_response[\"meta\"][\"next_token\"])\n",
    "                                    #print(json_response[\"meta\"][\"next_token\"])\n",
    "                            counter = 0\n",
    "                            tweets_counter = 0\n",
    "                            tweets_counter += json_response[\"meta\"][\"result_count\"]\n",
    "                            print(\"Next token value is: %s\" % json_response[\"meta\"][\"next_token\"])\n",
    "                            # sample limit is 500, so need only 500 tweets\n",
    "                            while json_response[\"meta\"][\"next_token\"] and tweets_counter <=sample_limit:\n",
    "                                sleep(2)\n",
    "                                json_response = connect_to_endpoint(search_url, query_params)\n",
    "\n",
    "\n",
    "                                json_file.writelines(\n",
    "                                \",\" + json.dumps(json_response[\"data\"])[1:-1])\n",
    "\n",
    "                                #print(json_response[\"meta\"][\"oldest_id\"] +\n",
    "                                #    \"\\n\" + json_response[\"meta\"][\"newest_id\"] +\n",
    "                                #   \"\\n\" + json_response[\"meta\"][\"next_token\"])\n",
    "\n",
    "                                print(\"Tweet count is: %s\" %  tweets_counter)\n",
    "                                print(\"Next token value is: %s\" %  json_response[\"meta\"][\"next_token\"])\n",
    "\n",
    "                                counter += 1\n",
    "                                tweets_counter += json_response[\"meta\"][\"result_count\"]\n",
    "\n",
    "\n",
    "\n",
    "                json_file.writelines(\"]}\")\n",
    "                json_file.close()\n",
    "            else:\n",
    "                print(\"Tweets not found\")\n",
    "    except Exception as e:\n",
    "        print(\"Error! error message is: %s\" % str(e))\n",
    "\n",
    "# check if connected to twitter api successfully\n",
    "if __name__ == \"__main__\":\n",
    "    #below function will extract tweets\n",
    "    get_tweets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cad11e6a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "079b42ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8fc72d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import requests\n",
    "import json\n",
    "from time import sleep\n",
    "\n",
    "bearer_token = \"AAAAAAAAAAAAAAAAAAAAANfsjwEAAAAAB2B3Qrvw%2Bva9BE6eBjThntmEfyY%3DecXMXHlAMedee4N7VRKx6rgw8VkUxaNoOdTgZJdhMu5KKf1mUH\"\n",
    "META_FILE = \"meta_file1.txt\"\n",
    "FILE = 'response04.json'\n",
    "\n",
    "search_url = \"https://api.twitter.com/2/tweets/search/recent\"\n",
    "# SEARCH_URL = \"https://api.twitter.com/2/tweets/search/all\"\n",
    "\n",
    "# Optional params: start_time,end_time,since_id,until_id,max_results,next_token,\n",
    "\n",
    "#query_params = {'query': '(from:LeoVaradkar -is:retweet)','tweet.fields': 'author_id'}\n",
    "query_params = {'query': '\"milk quality\" -is:retweet','tweet.fields': 'author_id', 'max_results': '100','next_token': {}}\n",
    "\n",
    "\n",
    "# twitter GET request parameters\n",
    "#query_params = {\n",
    "#    'query': 'hi',\n",
    "#    # 'max_results': 100,\n",
    "#    'tweet.fields': 'attachments,author_id,context_annotations,conversation_id,created_at,entities,geo,id,in_reply_to_user_id,lang,possibly_sensitive,referenced_tweets,reply_settings,source,text,withheld',\n",
    "#    'expansions': 'attachments.poll_ids,attachments.media_keys,author_id,geo.place_id,in_reply_to_user_id,referenced_tweets.id,entities.mentions.username,referenced_tweets.id.author_id',\n",
    "#    'place.fields': 'contained_within,country,country_code,full_name,geo,id,name,place_type',\n",
    "#    'user.fields': 'created_at,description,entities,id,location,name,pinned_tweet_id,profile_image_url,protected,public_metrics,url,username,verified,withheld',\n",
    "#    'start_time': '2021-02-15T00:00:01.000Z',\n",
    "#    'end_time': '2021-02-16T23:59:59.000Z'\n",
    "#}\n",
    "\n",
    "\n",
    "def bearer_oauth(r):\n",
    "    r.headers = {\"Authorization\": \"Bearer {}\".format(bearer_token)}\n",
    "    r.headers[\"User-Agent\"] = \"v2RecentSearchPython\"\n",
    "    return r\n",
    "\n",
    "def connect_to_endpoint(url, params):\n",
    "    response = requests.get(url, auth=bearer_oauth, params=params)\n",
    "    #print(response.status_code)\n",
    "    if response.status_code != 200:\n",
    "        raise Exception(response.status_code, response.text)\n",
    "    return response.json()\n",
    "\n",
    "def get_tweets():\n",
    "    \"\"\"\n",
    "    Pull tweets and and put them in a text file.\n",
    "    # TODO 1: open a new json file when reaches xx mb size/yy tweets.\n",
    "    # TODO 2: upload tweets to Google Sheets document.\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "\n",
    "    json_response = connect_to_endpoint(search_url, query_params)\n",
    "    \n",
    "\n",
    "    json_file = open(FILE, mode='a')\n",
    "    json_file.write('{\"data\": [')\n",
    "    json_file.writelines(json.dumps(json_response[\"data\"])[1:-1])\n",
    "    \n",
    "    nextToken =\"\"\n",
    "\n",
    "    for post in json_response:\n",
    "        if 'meta' in post:\n",
    "            for d in json_response[\"meta\"]:\n",
    "                if 'next_token' in d:\n",
    "                    #print(json_response['meta']['next_token'])\n",
    "                    nextToken=json_response['meta']['next_token']\n",
    "                    \n",
    "#if nextToken !=\"\":\n",
    "    \n",
    "        \n",
    "    #print(json_response[\"meta\"][\"oldest_id\"] +\n",
    "    #      \"\\n\" + json_response[\"meta\"][\"newest_id\"] +\n",
    "    #      \"\\n\" + json_response[\"meta\"][\"next_token\"])\n",
    "    #print(json_response[\"meta\"][\"next_token\"])\n",
    "    print(\"Next token value is: %s\" % json_response[\"meta\"][\"next_token\"])\n",
    "    \n",
    "    counter = 0\n",
    "    tweets_counter = 0\n",
    "    tweets_counter += json_response[\"meta\"][\"result_count\"]\n",
    "\n",
    "    \n",
    "    while json_response[\"meta\"][\"next_token\"]:\n",
    "        sleep(2)\n",
    "        json_response = connect_to_endpoint(search_url, query_params)\n",
    "\n",
    "        \n",
    "        json_file.writelines(\n",
    "            \",\" + json.dumps(json_response[\"data\"])[1:-1])\n",
    "\n",
    "        #print(json_response[\"meta\"][\"oldest_id\"] +\n",
    "        #    \"\\n\" + json_response[\"meta\"][\"newest_id\"] +\n",
    "        #   \"\\n\" + json_response[\"meta\"][\"next_token\"])\n",
    "        \n",
    "        print(\"page number is: %s\" %  tweets_counter)\n",
    "        print(\"Next token value is: %s\" %  json_response[\"meta\"][\"next_token\"])\n",
    "\n",
    "        counter += 1\n",
    "        tweets_counter += json_response[\"meta\"][\"result_count\"]\n",
    "        \n",
    "\n",
    "        \n",
    "    json_file.writelines(\"]}\")\n",
    "    json_file.close()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    get_tweets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4188f6c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c26f5cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import library first\n",
    "import requests\n",
    "import json\n",
    "from time import sleep\n",
    "\n",
    "# please add bearer_token here\n",
    "bearer_token = \"AAAAAAAAAAAAAAAAAAAAANfsjwEAAAAAB2B3Qrvw%2Bva9BE6eBjThntmEfyY%3DecXMXHlAMedee4N7VRKx6rgw8VkUxaNoOdTgZJdhMu5KKf1mUH\"\n",
    "#bearer_token = \"Add bearer token here\"\n",
    "\n",
    "# set limit for the sample\n",
    "sample_limit =500\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# file to save tweeter response\n",
    "FILE = 'tweet_response.json'\n",
    "# twitter search api url\n",
    "search_url = \"https://api.twitter.com/2/tweets/search/recent\"\n",
    "#search_url = \"https://api.twitter.com/2/tweets/search/all\"\n",
    "\n",
    "#Finnish milk\n",
    "# add query paramters here\n",
    "#change params based on the endpoint you are using\n",
    " \n",
    "query_params = {'query': 'ireland milk -is:retweet',\n",
    "                'tweet.fields': 'author_id', \n",
    "                'max_results': '100',\n",
    "                'next_token': {}}\n",
    "\n",
    "#query_params = {'query': 'ireland milk quality lang:en -is:retweet',\n",
    " #               'tweet.fields': 'author_id', \n",
    "  #              'max_results': '100',\n",
    "   #             'next_token': {}}\n",
    "\n",
    "# function to add request header\n",
    "def bearer_oauth(r):\n",
    "    r.headers = {\"Authorization\": \"Bearer {}\".format(bearer_token)}\n",
    "    r.headers[\"User-Agent\"] = \"v2RecentSearchPython\"\n",
    "    return r\n",
    "# function to send request and recieve response\n",
    "def connect_to_endpoint(url, params):\n",
    "    response = requests.get(url, auth=bearer_oauth, params=params)\n",
    "    #print(response.status_code)\n",
    "    # check if response code is good or bad\n",
    "    if response.status_code != 200:\n",
    "        raise Exception(response.status_code, response.text)\n",
    "    return response.json()\n",
    "\n",
    "# main function to get tweets\n",
    "def get_tweets():\n",
    "    json_response = connect_to_endpoint(search_url, query_params)\n",
    "    \n",
    "    # add try block to catch the error\n",
    "    try:\n",
    "        for post in json_response:\n",
    "            print(json_response)\n",
    "            returnVal = json_response[\"meta\"][\"result_count\"]\n",
    "            #print(\"Tweet count is: %s\" %  returnVal)\n",
    "            if returnVal > 0:\n",
    "                \n",
    "                \n",
    "\n",
    "                json_file = open(FILE, mode='a')\n",
    "                #step up to write the file\n",
    "                json_file.write('{\"data\": [')\n",
    "                # get data from json node\n",
    "                json_file.writelines(json.dumps(json_response[\"data\"])[1:-1])\n",
    "                \n",
    "                \n",
    "\n",
    "                nextToken =\"\"\n",
    "                \n",
    "                # check if meta tag is present the json response\n",
    "                if 'meta' in post:\n",
    "                    for d in json_response[\"meta\"]:\n",
    "                        # print(d)\n",
    "                        if 'next_token' in d:\n",
    "                            #   print(\"Next token value iswewe: %s\" % json_response['meta']['next_token'])\n",
    "                            #print(json_response['meta']['next_token'])\n",
    "                            nextToken=json_response['meta']['next_token']\n",
    "                            #if nextToken ==\"\":\n",
    "                            #print(json_response[\"meta\"][\"oldest_id\"] +\n",
    "                                    #      \"\\n\" + json_response[\"meta\"][\"newest_id\"] +\n",
    "                                    #      \"\\n\" + json_response[\"meta\"][\"next_token\"])\n",
    "                                    #print(json_response[\"meta\"][\"next_token\"])\n",
    "                            counter = 0\n",
    "                            tweets_counter = 0\n",
    "                            tweets_counter += json_response[\"meta\"][\"result_count\"]\n",
    "                            print(\"Next token value is: %s\" % json_response[\"meta\"][\"next_token\"])\n",
    "                            # sample limit is 500, so need only 500 tweets\n",
    "                            while json_response[\"meta\"][\"next_token\"] and tweets_counter <=sample_limit:\n",
    "                                sleep(2)\n",
    "                                json_response = connect_to_endpoint(search_url, query_params)\n",
    "\n",
    "\n",
    "                                json_file.writelines(\n",
    "                                \",\" + json.dumps(json_response[\"data\"])[1:-1])\n",
    "\n",
    "                                #print(json_response[\"meta\"][\"oldest_id\"] +\n",
    "                                #    \"\\n\" + json_response[\"meta\"][\"newest_id\"] +\n",
    "                                #   \"\\n\" + json_response[\"meta\"][\"next_token\"])\n",
    "\n",
    "                                print(\"Tweet count is: %s\" %  tweets_counter)\n",
    "                                print(\"Next token value is: %s\" %  json_response[\"meta\"][\"next_token\"])\n",
    "\n",
    "                                counter += 1\n",
    "                                tweets_counter += json_response[\"meta\"][\"result_count\"]\n",
    "\n",
    "\n",
    "\n",
    "                json_file.writelines(\"]}\")\n",
    "                json_file.close()\n",
    "            else:\n",
    "                print(\"Tweets not found\")\n",
    "    except Exception as e:\n",
    "        print(\"Error! error message is: %s\" % str(e))\n",
    "\n",
    "# check if connected to twitter api successfully\n",
    "if __name__ == \"__main__\":\n",
    "    #below function will extract tweets\n",
    "    get_tweets()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
